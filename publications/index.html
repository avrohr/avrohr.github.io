<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Alexander von Rohr </title> <meta name="author" content="Alexander von Rohr"> <meta name="description" content="Alexander von Rohr is an embodied AI researcher at TU Munich, specializing in Bayesian optimization, robust reinforcement learning, and robot learning. "> <meta name="keywords" content="Bayesian optimization, machine learning, academic-website, portfolio-website"> <meta property="og:site_name" content="Alexander von Rohr"> <meta property="og:type" content="website"> <meta property="og:title" content="Alexander von Rohr | publications"> <meta property="og:url" content="https://avrohr.com/publications/"> <meta property="og:description" content="Alexander von Rohr is an embodied AI researcher at TU Munich, specializing in Bayesian optimization, robust reinforcement learning, and robot learning. "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content="Alexander von Rohr is an embodied AI researcher at TU Munich, specializing in Bayesian optimization, robust reinforcement learning, and robot learning. "> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Alexander von Rohr"
        },
        "url": "https://avrohr.com/publications/",
        "@type": "WebSite",
        "description": "Alexander von Rohr is an embodied AI researcher at TU Munich, specializing in Bayesian optimization, robust reinforcement learning, and robot learning.
",
        "headline": "publications",
        
        "sameAs": ["https://orcid.org/0000-0002-0005-0310", "https://scholar.google.com/citations?user=hB2Q6DUAAAAJ", "https://github.com/avrohr", "https://www.linkedin.com/in/alexander-von-rohr", "https://www.dynsyslab.org/team/", "https://dblp.org/pid/227/2460.html"],
        
        "name": "Alexander von Rohr",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://avrohr.com/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alexander</span> von Rohr </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <p>* denotes equal contribution. You can also find my articles on <a href="https://scholar.google.de/citations?user=hB2Q6DUAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a>.</p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.sciencedirect.com/journal/systems-and-control-letters" rel="external nofollow noopener" target="_blank">SCL</a> </abbr> </div> <div id="vonrohr2025robust" class="col-sm-8"> <div class="title">Robust Direct Data-Driven Control for Probabilistic Systems</div> <div class="author"> <em>Alexander von Rohr</em>, Dmitrii Likhachev, and Sebastian Trimpe </div> <div class="periodical"> <em>Systems &amp; Control Letters</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.sysconle.2024.106011" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2306.16973" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0167691124002998" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S0167691124002998/pdfft?md5=105c4f5feb9b6f985b5ac80f7b0e2d76&amp;pid=1-s2.0-S0167691124002998-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Data-Science-in-Mechanical-Engineering/rddc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a data-driven control method for systems with aleatoric uncertainty, such as robot fleets with variations between agents. Our method leverages shared trajectory data to increase the robustness of the designed controller and thus facilitate transfer to new variations without the need for prior parameter and uncertainty estimation. In contrast to existing work on experience transfer for performance, our approach focuses on robustness and uses data collected from multiple realizations to guarantee generalization to unseen ones. Our method is based on scenario optimization combined with recent formulations for direct data-driven control. We derive upper bounds on the minimal amount of data required to provably achieve quadratic stability for probabilistic systems with aleatoric uncertainty and demonstrate the benefits of our data-driven method through a numerical example. We find that the learned controllers generalize well to high variations in the dynamics even when based on only a few short open-loop trajectories. Robust experience transfer enables the design of safe and robust controllers that work “out of the box” without additional learning during deployment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vonrohr2025robust</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{von Rohr}, Alexander and Likhachev, Dmitrii and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust Direct Data-Driven Control for Probabilistic Systems}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Systems &amp; Control Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{196}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{106011}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0167-6911}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.sysconle.2024.106011}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://jmlr.org/tmlr/" rel="external nofollow noopener" target="_blank">TMLR</a> </abbr> </div> <div id="brunzema2025eventtriggered" class="col-sm-8"> <div class="title">Event-Triggered Time-Varying Bayesian Optimization</div> <div class="author"> Paul Brunzema, <em>Alexander von Rohr</em>, Friedrich Solowjow, and Sebastian Trimpe </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.10790" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=WEYMCLu8u7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=WEYMCLu8u7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/brunzema/et-bo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Current approaches to TVBO require prior knowledge of a constant rate of change to cope with stale data arising from time variations. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function and then resets the dataset. This allows the algorithm to adapt online to realized temporal changes without the need for exact prior knowledge. The event trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We derive regret bounds for adaptive resets without exact prior knowledge of the temporal changes and show in numerical experiments that ET-GP-UCB outperforms competing GP-UCB algorithms on both synthetic and real-world data. The results demonstrate that ET-GP-UCB is readily applicable without extensive hyperparameter tuning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">brunzema2025eventtriggered</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Event-Triggered Time-Varying Bayesian Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brunzema, Paul and {von Rohr}, Alexander and Solowjow, Friedrich and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.ieee-ras.org/publications/t-ro" rel="external nofollow noopener" target="_blank">T-RO</a> </abbr> </div> <div id="he2025simulation" class="col-sm-8"> <div class="title">Simulation-Aided Policy Tuning for Black-Box Robot Learning</div> <div class="author"> Shiming He, <em>Alexander von Rohr</em>, Dominik Baumann, Ji Xiang, and Sebastian Trimpe </div> <div class="periodical"> <em>IEEE Transactions on Robotics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TRO.2025.3539192" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2411.14246" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10874195" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/Data-Science-in-Mechanical-Engineering/hci-gibo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>How can robots learn and adapt to new tasks and situations with little data? Systematic exploration and simulation are crucial tools for efficient robot learning. We present a novel black-box policy search algorithm focused on data-efficient policy improvements. The algorithm learns directly on the robot and treats simulation as an additional information source to speed up the learning process. At the core of the algorithm, a probabilistic model learns the dependence of the policy parameters and the robot learning objective not only by performing experiments on the robot, but also by leveraging data from a simulator. This substantially reduces interaction time with the robot. Using this model, we can guarantee improvements with high probability for each policy update, thereby facilitating fast, goal-oriented learning. We evaluate our algorithm on simulated fine-tuning tasks and demonstrate the data-efficiency of the proposed dual-information source optimization algorithm. In a real robot learning experiment, we show fast and successful task learning on a robot manipulator with the aid of an imperfect simulator.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">he2025simulation</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{He, Shiming and {von Rohr}, Alexander and Baumann, Dominik and Xiang, Ji and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Robotics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Simulation-Aided Policy Tuning for Black-Box Robot Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-17}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TRO.2025.3539192}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://jmlr.org/tmlr/" rel="external nofollow noopener" target="_blank">TMLR</a> </abbr> </div> <div id="rath2024discovering" class="col-sm-8"> <div class="title">Discovering Model Structure of Dynamical Systems with Combinatorial Bayesian Optimization</div> <div class="author"> Lucas Rath, <em>Alexander von Rohr</em>, Andreas Schultze, Sebastian Trimpe, and Burkhard Corves </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=2iOOvQmJBK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/lucasrm25/Model-Structure-Selection-CBOSS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Deciding on a model structure is a fundamental problem in machine learning. In this paper we consider the problem of building a data-based model for dynamical systems from a library of discrete components. In addition to optimizing performance, we consider crash and inequality constraints that arise from additional requirements, such as real-time capability and model complexity. We address this task of model structure selection with a focus on dynamical systems and propose to search over potential model structures efficiently using a constrained combinatorial Bayesian Optimization (BO) algorithm. We propose expressive surrogate models suited for combinatorial domains and an acquisition function that can handle inequality and crash constraints. We provide simulated benchmark problems within the domain of equation discovery of nonlinear dynamical systems. Our method outperforms the state-of-the-art in constrained combinatorial optimization of black-box functions and has a favorable computational overhead compared to other BO methods. As a real-world application example, we apply our method to optimize the configuration of an electric vehicle’s digital twin while ensuring its real-time capability for the use in one of the world’s largest driving simulators.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rath2024discovering</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Discovering Model Structure of Dynamical Systems with Combinatorial Bayesian Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rath, Lucas and {von Rohr}, Alexander and Schultze, Andreas and Trimpe, Sebastian and Corves, Burkhard}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.degruyter.com/journal/key/auto/html#" rel="external nofollow noopener" target="_blank">at</a> </abbr> </div> <div id="vonrohr2024local" class="col-sm-8"> <div class="title">Local Bayesian Optimization for Controller Tuning with Crash Constraints</div> <div class="author"> <em>Alexander von Rohr</em>, David Stenger, Dominik Scheurenberg, and Sebastian Trimpe </div> <div class="periodical"> <em>at - Automatisierungstechnik</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1515/auto-2023-0181" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.degruyter.com/document/doi/10.1515/auto-2023-0181/html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Controller tuning is crucial for closed-loop performance but often involves manual adjustments. Although Bayesian optimization (BO) has been established as a data-efficient method for automated tuning, applying it to large and high-dimensional search spaces remains challenging. We extend a recently proposed local variant of BO to include crash constraints, where the controller can only be successfully evaluated in an a-priori unknown feasible region. We demonstrate the efficiency of the proposed method through simulations and hardware experiments. Our findings showcase the potential of local BO to enhance controller performance and reduce the time and resources necessary for tuning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vonrohr2024local</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Local Bayesian Optimization for Controller Tuning with Crash Constraints}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{von Rohr}, Alexander and Stenger, David and Scheurenberg, Dominik and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{at - Automatisierungstechnik}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1515/auto-2023-0181}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{281--292}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{72}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">preprint</abbr> </div> <div id="hausdorfer2024latent" class="col-sm-8"> <div class="title">Latent Action Priors From a Single Gait Cycle Demonstration for Online Imitation Learning</div> <div class="author"> Oliver Hausdörfer, <em>Alexander von Rohr</em>, Éric Lefort, and Angela P. Schoellig </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.03246" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://sites.google.com/view/latent-action-priors" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/OliEfr/latent-action-priors" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Deep Reinforcement Learning (DRL) in simulation often results in brittle and unrealistic learning outcomes. To push the agent towards more desirable solutions, prior information can be injected in the learning process through, for instance, reward shaping, expert data, or motion primitives. We propose an additional inductive bias for robot learning: latent actions learned from expert demonstration as priors in the action space. We show that these action priors can be learned from only a single open-loop gait cycle using a simple autoencoder. Using these latent action priors combined with established style rewards for imitation in DRL achieves above expert demonstration level of performance and leads to more desirable gaits. Further, action priors substantially improve the performance on transfer tasks, even leading to gait transitions for higher target speeds.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hausdorfer2024latent</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hausdörfer, Oliver and {von Rohr}, Alexander and Lefort, Éric and Schoellig, Angela P.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Latent Action Priors From a Single Gait Cycle Demonstration for Online Imitation Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">preprint</abbr> </div> <div id="romer2024diffusion" class="col-sm-8"> <div class="title">Diffusion Predictive Control with Constraints</div> <div class="author"> Ralf Römer, <em>Alexander von Rohr</em>, and Angela P. Schoellig </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.09342" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Diffusion models have recently gained popularity for policy learning in robotics due to their ability to capture high-dimensional and multimodal distributions. However, diffusion policies are inherently stochastic and typically trained offline, limiting their ability to handle unseen and dynamic conditions where novel constraints not represented in the training data must be satisfied. To overcome this limitation, we propose diffusion predictive control with constraints (DPCC), an algorithm for diffusion-based control with explicit state and action constraints that can deviate from those in the training data. DPCC uses constraint tightening and incorporates model-based projections into the denoising process of a trained trajectory diffusion model. This allows us to generate constraint-satisfying, dynamically feasible, and goal-reaching trajectories for predictive control. We show through simulations of a robot manipulator that DPCC outperforms existing methods in satisfying novel test-time constraints while maintaining performance on the learned control task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EWRL</abbr> </div> <div id="massiani2024viability" class="col-sm-8"> <div class="title">Viability of Future Actions: Robust Reinforcement Learning via Entropy Regularization</div> <div class="author"> Pierre-François Massiani<sup>*</sup>, <em>Alexander von Rohr<sup>*</sup></em>, Lukas Haverbeck, and Sebastian Trimpe </div> <div class="periodical"> <em>In Seventeenth European Workshop on Reinforcement Learning</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=zP9hpDEzPq" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Despite the many recent advances in reinforcement learning (RL), the question of learning policies that robustly satisfy state constraints under disturbances remains open. This paper reveals how robustness arises naturally by combining two common practices in unconstrained RL: entropy regularization and constraints penalization. Our results provide a method to learn robust policies, model-free and with standard popular algorithms. We begin by showing how entropy regularization biases the constrained RL problem towards maximizing the number of future viable actions, which is a form of robustness. Then, we relax the safety constraints via penalties to obtain an unconstrained RL problem, which we show approximates its constrained counterpart arbitrarily closely. We support our findings with illustrative examples and on popular RL benchmarks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">massiani2024viability</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Massiani, Pierre-Fran{\c{c}}ois and {von Rohr}, Alexander and Haverbeck, Lukas and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Viability of Future Actions: Robust Reinforcement Learning via Entropy Regularization}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Seventeenth European Workshop on Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://cdc2022.ieeecss.org/" rel="external nofollow noopener" target="_blank">CDC22</a> </abbr> </div> <div id="brunzema2022controller" class="col-sm-8"> <div class="title">On Controller Tuning with Time-Varying Bayesian Optimization</div> <div class="author"> Paul Brunzema<sup>*</sup>, <em>Alexander von Rohr<sup>*</sup></em>, and Sebastian Trimpe </div> <div class="periodical"> <em>In Proceedings of the IEEE Conference on Decision and Control</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/CDC51059.2022.9992649" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2207.11120" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9992649" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/brunzema/uitvbo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Changing conditions or environments can cause system dynamics to vary over time. To ensure optimal control performance, controllers should adapt to these changes. When the underlying cause and time of change is unknown, we need to rely on online data for this adaptation. In this paper, we will use time-varying Bayesian optimization (TVBO) to tune controllers online in changing environments using appropriate prior knowledge on the control objective and its changes. Two properties are characteristic of many online controller tuning problems: First, they exhibit incremental and lasting changes in the objective due to changes to the system dynamics, e.g., through wear and tear. Second, the optimization problem is convex in the tuning parameters. Current TVBO methods do not explicitly account for these properties, resulting in poor tuning performance and many unstable controllers through over-exploration of the parameter space. We propose a novel TVBO forgetting strategy using Uncertainty-Injection (UI), which incorporates the assumption of incremental and lasting changes. The control objective is modeled as a spatio-temporal Gaussian process (GP) with UI through a Wiener process in the temporal domain. Further, we explicitly model the convexity assumptions in the spatial dimension through GP models with linear inequality constraints. In numerical experiments, we show that our model outperforms the state-of-the-art method in TVBO, exhibiting reduced regret and fewer unstable parameter configurations. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">brunzema2022controller</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brunzema, Paul and {von Rohr}, Alexander and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On Controller Tuning with Time-Varying Bayesian Optimization}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE Conference on Decision and Control}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4046-4052}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CDC51059.2022.9992649}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://cdc2022.ieeecss.org/" rel="external nofollow noopener" target="_blank">CDC22</a> </abbr> </div> <div id="vonrohr2022improving" class="col-sm-8"> <div class="title">Improving the Performance of Robust Control through Event-Triggered Learning</div> <div class="author"> <em>Alexander von Rohr</em>, Friedrich Solowjow, and Sebastian Trimpe </div> <div class="periodical"> <em>In Proceedings of the IEEE Conference on Decision and Control</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/CDC51059.2022.9993350" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2207.14252" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9993350" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/avrohr/betl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Robust controllers ensure stability in feedback loops designed under uncertainty but at the cost of performance. Model uncertainty in time-invariant systems can be reduced by recently proposed learning-based methods, thus improving the performance of robust controllers using data. However, in practice, many systems also exhibit uncertainty in the form of changes over time, e.g., due to weight shifts or wear and tear, leading to decreased performance or instability of the learning-based controller. We propose an event-triggered learning algorithm that decides when to learn in the face of uncertainty in the LQR problem with rare or slow changes. Our key idea is to switch between robust and learned controllers. For learning, we first approximate the optimal length of the learning phase via Monte-Carlo estimations using a probabilistic model. We then design a statistical test for uncertain systems based on the moment-generating function of the LQR cost. The test detects changes in the system under control and triggers re-learning when control performance deteriorates due to system changes. We demonstrate improved performance over a robust controller baseline in a numerical example.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vonrohr2022improving</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{von Rohr}, Alexander and Solowjow, Friedrich and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving the Performance of Robust Control through Event-Triggered Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE Conference on Decision and Control}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3424-3430}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CDC51059.2022.9993350}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://l4dc.ethz.ch/" rel="external nofollow noopener" target="_blank">3rd_L4DC</a> </abbr> </div> <div id="vonrohr2021probabilistic" class="col-sm-8"> <div class="title">Probabilistic robust linear quadratic regulators with Gaussian processes</div> <div class="author"> <em>Alexander von Rohr</em>, Matthias Neumann-Brosig, and Sebastian Trimpe </div> <div class="periodical"> <em>In Proceedings of the 3rd Conference on Learning for Dynamics and Control</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2105.07668" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://proceedings.mlr.press/v144/rohr21a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="http://proceedings.mlr.press/v144/rohr21a/rohr21a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Data-Science-in-Mechanical-Engineering/prlqr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Probabilistic models such as Gaussian processes (GPs) are powerful tools to learn unknown dynamical systems from data for subsequent use in control design. While learning-based control has the potential to yield superior performance in demanding applications, robustness to uncertainty remains an important challenge. Since Bayesian methods quantify uncertainty of the learning results, it is natural to incorporate these uncertainties in a robust design. In contrast to most state-of-the-art approaches that consider worst-case estimates, we leverage the learning methods’ posterior distribution in the controller synthesis. The result is a more informed and thus efficient trade-off between performance and robustness. We present a novel controller synthesis for linearized GP dynamics that yields robust controllers with respect to a probabilistic stability margin. The formulation is based on a recently proposed algorithm for linear quadratic control synthesis, which we extend by giving probabilistic robustness guarantees in the form of credibility bounds for the system’s stability. Comparisons to existing methods based on worst-case and certainty-equivalence designs reveal superior performance and robustness properties of the proposed method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vonrohr2021probabilistic</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{von Rohr}, Alexander and Neumann-Brosig, Matthias and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Probabilistic robust linear quadratic regulators with Gaussian processes}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 3rd Conference on Learning for Dynamics and Control}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{324--335}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Jadbabaie, Ali and Lygeros, John and Pappas, George J. and A.&amp;nbsp;Parrilo, Pablo and Recht, Benjamin and Tomlin, Claire J. and Zeilinger, Melanie N.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{144}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://neurips.cc/Conferences/2021" rel="external nofollow noopener" target="_blank">NeurIPS_2021</a> </abbr> </div> <div id="mueller2021local" class="col-sm-8"> <div class="title">Local policy search with Bayesian optimization</div> <div class="author"> Sarah Müller<sup>*</sup>, <em>Alexander von Rohr<sup>*</sup></em>, and Sebastian Trimpe </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.11899" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper/2021/hash/ad0f7a25211abc3889cb0f420c85e671-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2106.11899" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Data-Science-in-Mechanical-Engineering/gibo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) aims to find an optimal policy by interaction with an environment. Consequently, learning complex behavior requires a vast number of samples, which can be prohibitive in practice. Nevertheless, instead of systematically reasoning and actively choosing informative samples, policy gradients for local search are often obtained from random perturbations. These random samples yield high variance estimates and hence are sub-optimal in terms of sample complexity. Actively selecting informative samples is at the core of Bayesian optimization, which constructs a probabilistic surrogate of the objective from past samples to reason about informative subsequent ones. In this paper, we propose to join both worlds. We develop an algorithm utilizing a probabilistic model of the objective function and its gradient. Based on the model, the algorithm decides where to query a noisy zeroth-order oracle to improve the gradient estimates. The resulting algorithm is a novel type of policy search method, which we compare to existing black-box algorithms. The comparison reveals improved sample complexity and reduced variance in extensive empirical evaluations on synthetic objectives. Further, we highlight the benefits of active sampling on popular RL benchmarks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mueller2021local</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{M\"{u}ller, Sarah and {von Rohr}, Alexander and Trimpe, Sebastian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Local policy search with Bayesian optimization}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{20708--20720}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://sites.google.com/robot-learning.org/corl2019" rel="external nofollow noopener" target="_blank">CoRL_2019</a> </abbr> </div> <div id="heim2020learnable" class="col-sm-8"> <div class="title">A Learnable Safety Measure</div> <div class="author"> Steve Heim<sup>*</sup>, <em>Alexander von Rohr<sup>*</sup></em>, Sebastian Trimpe, and Alexander Badri-Spröwitz </div> <div class="periodical"> <em>In Proceedings of the Conference on Robot Learning</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1809.03225" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://proceedings.mlr.press/v100/heim20a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="http://proceedings.mlr.press/v100/heim20a/heim20a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Failures are challenging for learning to control physical systems since they risk damage, time-consuming resets, and often provide little gradient information. Adding safety constraints to exploration typically requires a lot of prior knowledge and domain expertise. We present a safety measure which implicitly captures how the system dynamics relate to a set of failure states. Not only can this measure be used as a safety function, but also to directly compute the set of safe state-action pairs. Further, we show a model-free approach to learn this measure by active sampling using Gaussian processes. While safety can only be guaranteed after learning the safety measure, we show that failures can already be greatly reduced by using the estimated measure during learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">heim2020learnable</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Heim, Steve and {von Rohr}, Alexander and Trimpe, Sebastian and Badri-Spr\"{o}witz, Alexander}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Learnable Safety Measure}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Conference on Robot Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{627--639}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{100}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">preprint</abbr> </div> <div id="marco2020excursion" class="col-sm-8"> <div class="title">Excursion Search for Constrained Bayesian Optimization under a Limited Budget of Failures</div> <div class="author"> Alonso Marco, <em>Alexander von Rohr</em>, Dominik Baumann, José Miguel Hernández-Lobato, and Sebastian Trimpe </div> <div class="periodical"> <em>arXiv</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2005.07443" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>When learning to ride a bike, a child falls down a number of times before achieving the first success. As falling down usually has only mild consequences, it can be seen as a tolerable failure in exchange for a faster learning process, as it provides rich information about an undesired behavior. In the context of Bayesian optimization under unknown constraints (BOC), typical strategies for safe learning explore conservatively and avoid failures by all means. On the other side of the spectrum, non conservative BOC algorithms that allow failing may fail an unbounded number of times before reaching the optimum. In this work, we propose a novel decision maker grounded in control theory that controls the amount of risk we allow in the search as a function of a given budget of failures. Empirical validation shows that our algorithm uses the failures budget more efficiently in a variety of optimization experiments, and generally achieves lower regret, than state-of-the-art methods. In addition, we propose an original algorithm for unconstrained Bayesian optimization inspired by the notion of excursion sets in stochastic processes, upon which the failures-aware algorithm is built.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.iros2018.org/" rel="external nofollow noopener" target="_blank">IROS_2018</a> </abbr> </div> <div id="vonrohr2018gait" class="col-sm-8"> <div class="title">Gait Learning for Soft Microrobots Controlled by Light Fields</div> <div class="author"> <em>Alexander von Rohr</em>, Sebastian Trimpe, Alonso Marco, Peer Fischer, and Stefano Palagi </div> <div class="periodical"> <em>In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/IROS.2018.8594092" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1809.03225" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8594092" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Soft microrobots based on photoresponsive materials and controlled by light fields can generate a variety of different gaits. This inherent flexibility can be exploited to maximize their locomotion performance in a given environment and used to adapt them to changing conditions. Albeit, because of the lack of accurate locomotion models, and given the intrinsic variability among microrobots, analytical control design is not possible. Common data-driven approaches, on the other hand, require running prohibitive numbers of experiments and lead to very sample-specific results. Here we propose a probabilistic learning approach for light-controlled soft microrobots based on Bayesian Optimization (BO) and Gaussian Processes (GPs). The proposed approach results in a learning scheme that is data-efficient, enabling gait optimization with a limited experimental budget, and robust against differences among microrobot samples. These features are obtained by designing the learning scheme through the comparison of different GP priors and BO settings on a semi-synthetic data set. The developed learning scheme is validated in microrobot experiments, resulting in a 115% improvement in a microrobot’s locomotion performance with an experimental budget of only 20 tests. These encouraging results lead the way toward self-adaptive microrobotic systems based on light-controlled soft microrobots and probabilistic learning control.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vonrohr2018gait</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{von Rohr}, Alexander and Trimpe, Sebastian and Marco, Alonso and Fischer, Peer and Palagi, Stefano}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gait Learning for Soft Microrobots Controlled by Light Fields}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6199-6206}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS.2018.8594092}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Alexander von Rohr. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 07, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://rum.cronitor.io/script.js"></script> <script>window.cronitor=window.cronitor||function(){(window.cronitor.q=window.cronitor.q||[]).push(arguments)},cronitor("config",{clientKey:"7787a5cdb2630f07df03d92f6bdaed4f"});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-teaching",title:"teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"news-i-attended-the-eeci-international-graduate-school-on-control-course-from-data-to-decisions-the-scenario-approach-by-marco-c-campi-and-simone-garatti-it-was-a-blast",title:"I attended the EECI International Graduate School on Control course From Data to...",description:"",section:"News"},{id:"news-our-paper-probabilistic-robust-linear-quadratic-regulators-with-gaussian-processes-was-accepted-at-the-3rd-annual-learning-for-dynamics-amp-amp-control-conference",title:"Our paper Probabilistic robust linear quadratic regulators with Gaussian processes was accepted at...",description:"",section:"News"},{id:"news-i-presented-our-paper-probabilistic-robust-linear-quadratic-regulators-with-gaussian-processes-at-the-poster-session-of-the-the-3rd-annual-learning-for-dynamics-amp-amp-control-conference-thanks-to-everyone-who-stopped-by",title:"I presented our paper Probabilistic robust linear quadratic regulators with Gaussian processes at...",description:"",section:"News"},{id:"news-yesterday-we-published-a-preprint-on-local-policy-search-with-bayesian-optimization-it-is-about-active-sampling-for-policy-gradients-without-access-to-a-first-order-oracle",title:"Yesterday we published a preprint on Local policy search with Bayesian optimization. It...",description:"",section:"News"},{id:"news-our-paper-on-local-policy-search-with-bayesian-optimization-got-accepted-at-neurips-2021",title:"Our paper on Local policy search with Bayesian optimization got accepted at NeurIPS...",description:"",section:"News"},{id:"news-our-two-submissions-improving-the-performance-of-robust-control-through-event-triggered-learning-and-on-controller-tuning-with-time-varying-bayesian-optimization-for-the-ieee-conference-on-decision-and-control-cdc-were-accepted-we-will-present-the-papers-at-the-invited-session-on-learning-based-control",title:"Our two submissions Improving the Performance of Robust Control through Event-Triggered Learning and...",description:"",section:"News"},{id:"news-today-we-published-event-triggered-time-varying-bayesian-optimization-on-arxiv-the-paper-is-about-a-finding-the-optimum-of-a-time-varying-objective-function",title:"Today we published Event-Triggered Time-Varying Bayesian Optimization on arXiv. The paper is about...",description:"",section:"News"},{id:"news-i-m-excited-to-share-that-today-i-gave-a-talk-on-local-policy-search-with-bayesian-gradient-estimates-at-the-research-seminar-on-artificial-intelligence-which-was-organized-by-the-rwth-center-for-artificial-intelligence",title:"I\u2019m excited to share that today I gave a talk on Local Policy...",description:"",section:"News"},{id:"news-i-recently-attended-the-57th-control-engineering-colloquium-in-boppard-and-it-was-a-nice-platform-to-share-my-research-and-discuss-with-other-researchers-in-the-field-i-m-happy-to-announce-that-my-presentation-was-chosen-as-the-runner-up-for-the-best-presentation-award-making-the-experience-even-more-rewarding",title:"I recently attended the 57th Control Engineering Colloquium in Boppard and it was...",description:"",section:"News"},{id:"news-i-am-thrilled-to-share-that-my-former-master-s-student-and-now-colleague-paul-brunzema-has-won-the-2022-student-award-of-the-sew-eurodrive-foundation-for-our-work-on-learning-and-forgetting-in-time-varying-bayesian-optimization",title:"I am thrilled to share that my former Master\u2019s student and now colleague,...",description:"",section:"News"},{id:"news-today-i-gave-a-talk-on-local-search-with-bayesian-gradient-estimates-at-the-kick-off-event-for-the-center-for-algorithmics-and-optimization-at-the-rwth-aachen-university",title:"Today I gave a talk on Local Search with Bayesian Gradient Estimates at...",description:"",section:"News"},{id:"news-we-have-significantly-improved-our-preprint-event-triggered-time-varying-bayesian-optimization-you-find-the-new-version-on-arxiv",title:"We have significantly improved our preprint Event-Triggered Time-Varying Bayesian Optimization. You find the...",description:"",section:"News"},{id:"news-have-you-ever-wondered-how-to-make-your-data-driven-controllers-more-robust-to-variations-we-just-published-a-preprint-on-this-topic-experience-transfer-for-robust-direct-data-driven-control",title:"Have you ever wondered how to make your data-driven controllers more robust to...",description:"",section:"News"},{id:"news-today-we-present-our-work-on-experience-transfer-for-robust-direct-data-driven-control-at-the-poster-session-of-the-rwth-aachen-ai-colloquium",title:"Today we present our work on Experience Transfer for Robust Direct Data-Driven Control...",description:"",section:"News"},{id:"news-our-new-paper-on-discovering-model-structure-of-dynamical-systems-with-combinatorial-bayesian-optimization-has-been-published-at-the-transactions-on-machine-learning-research",title:"Our new paper on Discovering Model Structure of Dynamical Systems with Combinatorial Bayesian...",description:"",section:"News"},{id:"news-i-started-a-new-position-with-the-learning-systems-and-robotics-lab-at-the-technical-university-of-munich",title:"I started a new position with the Learning Systems and Robotics Lab at...",description:"",section:"News"},{id:"news-our-new-paper-on-local-bayesian-optimization-for-controller-tuning-with-crash-constraints-has-been-published-in-the-journal-at-automatisierungstechnik",title:"Our new paper on Local Bayesian Optimization for Controller Tuning with Crash Constraints...",description:"",section:"News"},{id:"news-i-m-excited-to-be-presenting-our-new-work-on-viability-of-future-actions-robust-reinforcement-learning-via-entropy-regularization-at-the-european-workshop-on-reinforcement-learning-in-toulouse-france-from-28-30-october-2024-looking-forward-to-great-discussions",title:"I\u2019m excited to be presenting our new work on Viability of Future Actions:...",description:"",section:"News"},{id:"news-next-week-i-ll-be-at-the-2024-conference-on-robot-learning-representing-the-robotics-institute-germany-in-the-exhibition-hall-we-ll-also-be-presenting-our-work-on-latent-action-priors-from-a-single-gait-cycle-demonstration-for-online-imitation-learning-at-the-locolearn-from-bioinspired-gait-generation-to-active-perception-workshop-and-fine-tuning-of-neural-network-approximate-mpc-without-retraining-via-bayesian-optimization-at-the-safe-rol-safe-and-robust-robot-learning-for-operation-in-the-real-world-workshop",title:"Next week, I\u2019ll be at the 2024 Conference on Robot Learning, representing the...",description:"",section:"News"},{id:"news-our-new-preprint-on-simulation-aided-policy-tuning-for-black-box-robot-learning-is-finally-online-this-research-tackles-the-challenge-of-data-efficient-fine-tuning-of-robot-behaviors-building-on-our-prior-work-we-propose-a-local-bayesian-optimization-algorithm-that-leverages-both-robot-experiments-and-simulation-to-speed-up-learning-check-out-the-preprint-and-the-video-for-more-details",title:"Our new preprint on Simulation-Aided Policy Tuning for Black-Box Robot Learning is finally...",description:"",section:"News"},{id:"news-today-marks-a-big-personal-milestone-i-successfully-defended-my-dissertation-titled-probabilistic-optimization-for-the-control-of-dynamical-systems-at-the-rwth-aachen-university",title:"Today marks a big personal milestone: I successfully defended my dissertation titled Probabilistic...",description:"",section:"News"},{id:"news-i-m-excited-to-share-our-paper-robust-direct-data-driven-control-for-probabilistic-systems-is-now-published-in-systems-amp-amp-control-letters-this-work-introduces-a-method-for-robust-experience-transfer-enabling-learning-based-controller-designs-shared-over-multiple-systems-using-just-a-few-trajectories-our-approach-guarantees-robustness-and-ensures-safe-operation-out-of-the-box",title:"I\u2019m excited to share our paper Robust Direct Data-Driven Control for Probabilistic Systems...",description:"",section:"News"},{id:"news-pleased-to-announce-that-our-latest-paper-event-triggered-time-varying-bayesian-optimization-is-published-in-the-transactions-on-machine-learning-research-this-work-introduces-et-gp-ucb-an-algorithm-that-adaptively-resets-its-optimization-process-in-response-to-changes-in-a-time-varying-objective-function-achieving-efficient-performance-without-prior-knowledge-of-change-rates",title:"Pleased to announce that our latest paper, Event-Triggered Time-Varying Bayesian Optimization, is published...",description:"",section:"News"},{id:"news-our-work-on-simulation-aided-policy-tuning-for-black-box-robot-learning-has-been-published-in-the-ieee-transactions-on-robotics-t-ro-and-is-now-available-in-early-access-our-new-approach-combines-real-world-amp-amp-simulated-data-to-fine-tune-robot-skills-efficiently-see-it-in-action-https-youtu-be-femkxdngxl4",title:"Our work on Simulation-Aided Policy Tuning for Black-Box Robot Learning has been published...",description:"",section:"News"},{id:"news-i-presented-our-work-on-viability-of-future-actions-robust-reinforcement-learning-via-entropy-regularization-at-the-second-mini-workshop-on-reinforcement-learning-in-mannheim",title:"I presented our work on Viability of Future Actions: Robust Reinforcement Learning via...",description:"",section:"News"},{id:"news-our-paper-on-diffusion-predictive-control-with-constraints-got-accepted-at-the-7th-annual-learning-for-dynamics-amp-amp-control-conference-l4dc",title:"Our paper on Diffusion Predictive Control with Constraints got accepted at the 7th...",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6C%65%78.%76%6F%6E.%72%6F%68%72@%74%75%6D.%64%65","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-0005-0310","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=hB2Q6DUAAAAJ","_blank")}},{id:"socials-ieee-xplore",title:"IEEE Xplore",section:"Socials",handler:()=>{window.open("https://ieeexplore.ieee.org/author/37086578278/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/avrohr","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/alexander-von-rohr","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://www.dynsyslab.org/team/","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/227/2460.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>